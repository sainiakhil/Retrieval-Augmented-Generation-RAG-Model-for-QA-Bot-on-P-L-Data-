{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNblR1OOCw1dd57Cd7vGi8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainiakhil/Retrieval-Augmented-Generation-RAG-Model-for-QA-Bot-on-P-L-Data-/blob/main/Part_2_Interactive_QA_Bot_Interface_for_Financial_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4IXLSdlkMSx"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install llama-index\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os"
      ],
      "metadata": {
        "id": "gt_A-x5jlARU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2m3INDfD7mYEcHw8VB1STZuofFc_UBjx4mHCjjpu5iZbbxgN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0t6mO7GglCiM",
        "outputId": "2b6e77ca-38eb-42b4-d21a-cb2731a6543e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    Settings\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "import tempfile\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Embedding Model\n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "    llm = HuggingFaceLLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=128,  # Reduce token generation\n",
        "    )\n",
        "\n",
        "    return llm, embed_model\n",
        "\n",
        "@st.cache_resource\n",
        "def create_vector_store(_docs, _settings_obj):\n",
        "  vector_index = VectorStoreIndex.from_documents(\n",
        "      _docs,\n",
        "      embed_model=_settings_obj.embed_model,\n",
        "      node_parser=_settings_obj.node_parser,\n",
        "      show_progress=True\n",
        "      )\n",
        "  return vector_index\n",
        "\n",
        "def main():\n",
        "    st.title(\"Financial Document Q&A Assistant\")\n",
        "\n",
        "    # Sidebar for document upload\n",
        "    st.sidebar.header(\"Upload Financial Documents\")\n",
        "    uploaded_files = st.sidebar.file_uploader(\n",
        "        \"Choose PDF files\",\n",
        "        type=\"pdf\",\n",
        "        accept_multiple_files=True\n",
        "    )\n",
        "\n",
        "    # Load model and embedding\n",
        "    llm, embed_model = load_model()\n",
        "\n",
        "    # Configure Settings\n",
        "    settings = Settings\n",
        "    settings.llm = llm\n",
        "    settings.embed_model = embed_model\n",
        "    settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "    settings.num_output = 50\n",
        "    settings.context_window = 2048\n",
        "    settings.genrate_kwargs = {\"do_sample\": False,\"temperature\": 0.1, \"max_new_tokens\": 128}\n",
        "\n",
        "    # Process uploaded documents\n",
        "    if uploaded_files:\n",
        "        # Create temporary directory\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            # Save uploaded files\n",
        "            doc_paths = []\n",
        "            for file in uploaded_files:\n",
        "                file_path = os.path.join(temp_dir, file.name)\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(file.getvalue())\n",
        "                doc_paths.append(file_path)\n",
        "\n",
        "            # Load documents\n",
        "            reader = SimpleDirectoryReader(input_dir=temp_dir)\n",
        "            documents = reader.load_data()\n",
        "\n",
        "            # Create vector store and index\n",
        "            index = create_vector_store(documents, settings)\n",
        "\n",
        "            # Query Engine\n",
        "            query_engine = index.as_query_engine(\n",
        "                llm=settings.llm,\n",
        "                similarity_top_k= 5,\n",
        "                response_mode=\"compact\",\n",
        "                verbose=True,\n",
        "                generate_kwargs=settings.genrate_kwargs,\n",
        "                context_window=settings.context_window,\n",
        "                num_output=settings.num_output,\n",
        "                show_progress=True\n",
        "              )\n",
        "\n",
        "            # Query input\n",
        "            query = st.text_input(\"Enter your financial query:\")\n",
        "\n",
        "            # Display results\n",
        "            if query:\n",
        "                with st.spinner(\"Analyzing documents...\"):\n",
        "                    response = query_engine.query(query)\n",
        "                    cleaned_response = response.response.split('\\n')[0].strip()\n",
        "\n",
        "                st.header(\"Original Query:\", divider=\"rainbow\")\n",
        "                st.header(query)\n",
        "                st.subheader(\"Response:\")\n",
        "                st.write(cleaned_response)\n",
        "\n",
        "                st.header(\"Retrieved Document Chunks:\", divider = True)\n",
        "                for i, node in enumerate(response.source_nodes, 1):\n",
        "                    st.text(f\"Chunk {i}:\")\n",
        "                    st.text(f\"Relevance Score: {node.score:.4f}\")\n",
        "                    st.text(f\"Text (first 300 chars): {node.text[:300]}...\")\n",
        "                    st.text(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QI1qQDs6lFMH",
        "outputId": "fd23f378-c722-4949-d221-418b0dc15c83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ngrok to expose the Streamlit app to the public\n",
        "public_url = ngrok.connect(addr='8501', proto = 'http',bind_tls = True)\n",
        "print(f'Streamlit app will be live at: {public_url}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gBgOUdVolO_P",
        "outputId": "d6756523-83f6-4ed3-b166-37a40f9f7a1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app will be live at: NgrokTunnel: \"https://4f4d-34-16-165-240.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZUh82xs6lSRr",
        "outputId": "01f23152-d934-4c16-fca6-2a8dba02e503"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.165.240:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-01-25 13:30:53.056556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-25 13:30:53.080126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-25 13:30:53.087230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-25 13:30:53.104377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-25 13:30:54.815006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 2/2 [00:30<00:00, 15.25s/it]\n",
            "2025-01-25 13:31:32.623 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Parsing nodes: 100% 40/40 [00:00<00:00, 298.34it/s]\n",
            "Generating embeddings: 100% 109/109 [00:01<00:00, 75.69it/s] \n",
            "2025-01-25 13:31:59.511 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-01-25 13:32:41.477 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}